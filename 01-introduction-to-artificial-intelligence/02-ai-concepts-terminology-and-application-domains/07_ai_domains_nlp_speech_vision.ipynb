{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b994fb71",
   "metadata": {},
   "source": [
    "# AI Domains ‚Äî NLP, Speech, and Computer Vision\n",
    "\n",
    "This notebook explores the **three practical domains of AI** ‚Äî Natural Language Processing (NLP), Speech Recognition, and Computer Vision.\n",
    "\n",
    "You'll see how models interpret human language, convert speech to text, and perceive the visual world ‚Äî forming the foundation of human-AI interaction.\n",
    "\n",
    "---\n",
    "### Objectives\n",
    "- Define NLP, Speech, and Vision in AI.\n",
    "- Perform text tokenization, sentiment analysis, and entity recognition.\n",
    "- Demonstrate basic speech recognition and text-to-speech synthesis.\n",
    "- Apply pre-trained neural networks for image recognition.\n",
    "- Understand how these domains integrate into multimodal AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545e299",
   "metadata": {},
   "source": [
    "## üí¨ 1. Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing enables computers to **understand, interpret, and generate human language**.\n",
    "\n",
    "NLP involves several key subfields:\n",
    "- **Tokenization:** Breaking sentences into smaller parts (tokens).\n",
    "- **Part-of-Speech Tagging:** Identifying grammatical roles.\n",
    "- **Named Entity Recognition (NER):** Detecting entities like people, organizations, or locations.\n",
    "- **Sentiment Analysis:** Determining emotional tone.\n",
    "- **Language Generation:** Producing text that reads naturally.\n",
    "\n",
    "Let's see this in action using Hugging Face‚Äôs pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56942998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Example text\n",
    "text = \"IBM Watson and OpenAI are transforming the AI landscape in healthcare and finance.\"\n",
    "\n",
    "# Named Entity Recognition\n",
    "ner = pipeline('ner', grouped_entities=True)\n",
    "entities = ner(text)\n",
    "print('Named Entities:', entities)\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentiment = pipeline('sentiment-analysis')\n",
    "result = sentiment(\"Artificial intelligence is revolutionizing technology.\")\n",
    "print('Sentiment:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d29d3f",
   "metadata": {},
   "source": [
    "### üß† Insights\n",
    "NLP systems combine statistical, neural, and linguistic models to interpret meaning and emotion. They enable chatbots, translators, summarizers, and search engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844fe029",
   "metadata": {},
   "source": [
    "## üó£Ô∏è 2. Speech Recognition and Synthesis\n",
    "\n",
    "**Speech-to-Text (STT):** Converts spoken language into text.\n",
    "**Text-to-Speech (TTS):** Converts written text into spoken audio.\n",
    "\n",
    "Together, these create seamless voice interfaces ‚Äî used in assistants like Siri, Alexa, and Google Assistant.\n",
    "\n",
    "We'll simulate both STT and TTS locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Speech demonstration\n",
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 160)\n",
    "engine.say('Welcome to Artificial Intelligence speech synthesis demonstration.')\n",
    "engine.runAndWait()\n",
    "print('‚úÖ Text spoken successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac59073",
   "metadata": {},
   "source": [
    "### üß† STT Concept (Simulated Example)\n",
    "\n",
    "Real STT would use libraries like `SpeechRecognition` or APIs like Google Speech.\n",
    "Here‚Äôs a simulated transcription pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ab5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_stt(audio_input):\n",
    "    print(f\"[Simulated Recognition] Input: {audio_input}\")\n",
    "    text = \"AI systems are enhancing communication.\"\n",
    "    print(f\"Transcribed Output: {text}\")\n",
    "    return text\n",
    "\n",
    "simulated_stt('sample_audio.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a06cbd",
   "metadata": {},
   "source": [
    "### üîÑ Integration Example\n",
    "1. **STT:** Captures user voice input ‚Üí converts to text.\n",
    "2. **NLP:** Interprets text for intent/sentiment.\n",
    "3. **TTS:** Responds back to the user in natural speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa259e1",
   "metadata": {},
   "source": [
    "## üëÅÔ∏è 3. Computer Vision (CV)\n",
    "\n",
    "Computer Vision enables AI to interpret visual data ‚Äî images and videos.\n",
    "It involves three main tasks:\n",
    "\n",
    "| Task | Description | Example |\n",
    "|------|--------------|----------|\n",
    "| **Image Classification** | Identify object type | Cat vs Dog |\n",
    "| **Object Detection** | Locate multiple objects | Cars, People in frame |\n",
    "| **Segmentation** | Label every pixel | Medical imaging, AR |\n",
    "\n",
    "We'll use a pre-trained **ResNet-18** model for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load pre-trained ResNet\n",
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "model.eval()\n",
    "\n",
    "# Load an example image\n",
    "url = 'https://upload.wikimedia.org/wikipedia/commons/9/9a/Pug_600.jpg'\n",
    "img = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Transform for model\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "input_tensor = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "predicted_idx = torch.argmax(output[0]).item()\n",
    "print(f\"Predicted class index: {predicted_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db62aa6b",
   "metadata": {},
   "source": [
    "### üß† Vision System Insights\n",
    "CNNs like ResNet and YOLO detect and interpret visual information efficiently.\n",
    "Applications: autonomous driving, surveillance, quality inspection, and AR/VR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e1f0c",
   "metadata": {},
   "source": [
    "## üîó 4. Integrating NLP, Speech, and Vision\n",
    "\n",
    "Modern AI systems combine multiple modalities ‚Äî **language, audio, and vision** ‚Äî to create intelligent assistants and multimodal agents.\n",
    "\n",
    "Example pipeline:\n",
    "1. **User speaks a query ‚Üí STT ‚Üí Text**\n",
    "2. **NLP ‚Üí Intent Detection ‚Üí Context Understanding**\n",
    "3. **CV ‚Üí Visual Confirmation or Scene Understanding**\n",
    "4. **TTS ‚Üí Response Output (Human-like)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebabb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "g = Digraph('AIDomains', format='png')\n",
    "g.attr(rankdir='LR', size='9,4')\n",
    "g.attr('node', shape='box', style='filled', fillcolor='lightgreen')\n",
    "g.node('Speech', 'Speech Input (STT)')\n",
    "g.node('NLP', 'Text Understanding (NLP)')\n",
    "g.node('Vision', 'Scene Analysis (CV)')\n",
    "g.node('Response', 'Response Generation (TTS)')\n",
    "g.edges([('Speech','NLP'), ('NLP','Vision'), ('Vision','Response')])\n",
    "g.attr(label='Multimodal AI Pipeline: Voice ‚Üí Text ‚Üí Vision ‚Üí Voice')\n",
    "g.render('ai_domains_pipeline', view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad42d8",
   "metadata": {},
   "source": [
    "## üìò 5. Summary and Insights\n",
    "\n",
    "- **NLP** enables understanding and generation of human language.\n",
    "- **Speech AI** bridges human voice and digital understanding (STT ‚Üî TTS).\n",
    "- **Computer Vision** empowers perception and spatial awareness.\n",
    "- Combined, they form the foundation of **multimodal AI systems** ‚Äî powering assistants, self-driving cars, and smart robots.\n",
    "\n",
    "Next: In the following notebook, we'll explore how **AI integrates with Cloud, Edge, and IoT ecosystems** to deliver scalable real-world intelligence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
