{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b994fb71",
   "metadata": {},
   "source": [
    "# AI Domains ‚Äî NLP, Speech, and Computer Vision\n",
    "\n",
    "This notebook explores the **three practical domains of AI** ‚Äî Natural Language Processing (NLP), Speech Recognition, and Computer Vision.\n",
    "\n",
    "You'll see how models interpret human language, convert speech to text, and perceive the visual world ‚Äî forming the foundation of human-AI interaction.\n",
    "\n",
    "---\n",
    "### Objectives\n",
    "- Define NLP, Speech, and Vision in AI.\n",
    "- Perform text tokenization, sentiment analysis, and entity recognition.\n",
    "- Demonstrate basic speech recognition and text-to-speech synthesis.\n",
    "- Apply pre-trained neural networks for image recognition.\n",
    "- Understand how these domains integrate into multimodal AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545e299",
   "metadata": {},
   "source": [
    "## üí¨ 1. Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing enables computers to **understand, interpret, and generate human language**.\n",
    "\n",
    "NLP involves several key subfields:\n",
    "- **Tokenization:** Breaking sentences into smaller parts (tokens).\n",
    "- **Part-of-Speech Tagging:** Identifying grammatical roles.\n",
    "- **Named Entity Recognition (NER):** Detecting entities like people, organizations, or locations.\n",
    "- **Sentiment Analysis:** Determining emotional tone.\n",
    "- **Language Generation:** Producing text that reads naturally.\n",
    "\n",
    "Let's see this in action using Hugging Face‚Äôs pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56942998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: [{'entity_group': 'ORG', 'score': np.float32(0.9604447), 'word': 'IBM Watson', 'start': 0, 'end': 10}, {'entity_group': 'ORG', 'score': np.float32(0.9956719), 'word': 'OpenAI', 'start': 15, 'end': 21}, {'entity_group': 'MISC', 'score': np.float32(0.7745123), 'word': 'AI', 'start': 43, 'end': 45}]\n",
      "Sentiment: [{'label': 'POSITIVE', 'score': 0.9988914132118225}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Example text\n",
    "text = \"IBM Watson and OpenAI are transforming the AI landscape in healthcare and finance.\"\n",
    "\n",
    "# Named Entity Recognition\n",
    "ner = pipeline('ner', grouped_entities=True)\n",
    "entities = ner(text)\n",
    "print('Named Entities:', entities)\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentiment = pipeline('sentiment-analysis')\n",
    "result = sentiment(\"Artificial intelligence is revolutionizing technology.\")\n",
    "print('Sentiment:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d29d3f",
   "metadata": {},
   "source": [
    "### üß† Insights\n",
    "NLP systems combine statistical, neural, and linguistic models to interpret meaning and emotion. They enable chatbots, translators, summarizers, and search engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844fe029",
   "metadata": {},
   "source": [
    "## üó£Ô∏è 2. Speech Recognition and Synthesis\n",
    "\n",
    "**Speech-to-Text (STT):** Converts spoken language into text.\n",
    "**Text-to-Speech (TTS):** Converts written text into spoken audio.\n",
    "\n",
    "Together, these create seamless voice interfaces ‚Äî used in assistants like Siri, Alexa, and Google Assistant.\n",
    "\n",
    "We'll simulate both STT and TTS locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "006b0e11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pywintypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyttsx3\\__init__.py:22\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(driverName, debug)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     eng = \u001b[43m_activeEngines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdriverName\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\weakref.py:136\u001b[39m, in \u001b[36mWeakValueDictionary.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28mself\u001b[39m._commit_removals()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m o = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m()\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m o \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: None",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Text-to-Speech demonstration\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyttsx3\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m engine = \u001b[43mpyttsx3\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m engine.setProperty(\u001b[33m'\u001b[39m\u001b[33mrate\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m160\u001b[39m)\n\u001b[32m      6\u001b[39m engine.say(\u001b[33m'\u001b[39m\u001b[33mWelcome to Artificial Intelligence speech synthesis demonstration.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyttsx3\\__init__.py:24\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(driverName, debug)\u001b[39m\n\u001b[32m     22\u001b[39m     eng = _activeEngines[driverName]\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     eng = \u001b[43mEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriverName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     _activeEngines[driverName] = eng\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m eng\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyttsx3\\engine.py:59\u001b[39m, in \u001b[36mEngine.__init__\u001b[39m\u001b[34m(self, driverName, debug)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33;03mConstructs a new TTS engine instance.\u001b[39;00m\n\u001b[32m     51\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m \u001b[33;03m@type debug: bool\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m.driver_name = driverName \u001b[38;5;129;01mor\u001b[39;00m default_engine_by_sys_platform()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28mself\u001b[39m.proxy = \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDriverProxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m._connects = {}\n\u001b[32m     61\u001b[39m \u001b[38;5;28mself\u001b[39m._debug = debug\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyttsx3\\driver.py:41\u001b[39m, in \u001b[36mDriverProxy.__init__\u001b[39m\u001b[34m(self, engine, driverName, debug)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m driverName\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# import driver module\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28mself\u001b[39m._module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyttsx3.drivers.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdriverName\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# build driver instance\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mself\u001b[39m._driver = \u001b[38;5;28mself\u001b[39m._module.buildDriver(weakref.proxy(\u001b[38;5;28mself\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1026\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyttsx3\\drivers\\sapi5.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mweakref\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpythoncom\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyttsx3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvoice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Voice\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# common voices\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pythoncom.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Magic utility that \"redirects\" to pythoncomXX.dll\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpywintypes\u001b[39;00m\n\u001b[32m      4\u001b[39m pywintypes.__import_pywin32_system_module__(\u001b[33m\"\u001b[39m\u001b[33mpythoncom\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mglobals\u001b[39m())\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pywintypes'"
     ]
    }
   ],
   "source": [
    "# Text-to-Speech demonstration\n",
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 160)\n",
    "engine.say('Welcome to Artificial Intelligence speech synthesis demonstration.')\n",
    "engine.runAndWait()\n",
    "print('‚úÖ Text spoken successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac59073",
   "metadata": {},
   "source": [
    "### üß† STT Concept (Simulated Example)\n",
    "\n",
    "Real STT would use libraries like `SpeechRecognition` or APIs like Google Speech.\n",
    "Here‚Äôs a simulated transcription pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ab5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_stt(audio_input):\n",
    "    print(f\"[Simulated Recognition] Input: {audio_input}\")\n",
    "    text = \"AI systems are enhancing communication.\"\n",
    "    print(f\"Transcribed Output: {text}\")\n",
    "    return text\n",
    "\n",
    "simulated_stt('sample_audio.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a06cbd",
   "metadata": {},
   "source": [
    "### üîÑ Integration Example\n",
    "1. **STT:** Captures user voice input ‚Üí converts to text.\n",
    "2. **NLP:** Interprets text for intent/sentiment.\n",
    "3. **TTS:** Responds back to the user in natural speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa259e1",
   "metadata": {},
   "source": [
    "## üëÅÔ∏è 3. Computer Vision (CV)\n",
    "\n",
    "Computer Vision enables AI to interpret visual data ‚Äî images and videos.\n",
    "It involves three main tasks:\n",
    "\n",
    "| Task | Description | Example |\n",
    "|------|--------------|----------|\n",
    "| **Image Classification** | Identify object type | Cat vs Dog |\n",
    "| **Object Detection** | Locate multiple objects | Cars, People in frame |\n",
    "| **Segmentation** | Label every pixel | Medical imaging, AR |\n",
    "\n",
    "We'll use a pre-trained **ResNet-18** model for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load pre-trained ResNet\n",
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "model.eval()\n",
    "\n",
    "# Load an example image\n",
    "url = 'https://upload.wikimedia.org/wikipedia/commons/9/9a/Pug_600.jpg'\n",
    "img = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Transform for model\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "input_tensor = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "predicted_idx = torch.argmax(output[0]).item()\n",
    "print(f\"Predicted class index: {predicted_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db62aa6b",
   "metadata": {},
   "source": [
    "### üß† Vision System Insights\n",
    "CNNs like ResNet and YOLO detect and interpret visual information efficiently.\n",
    "Applications: autonomous driving, surveillance, quality inspection, and AR/VR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e1f0c",
   "metadata": {},
   "source": [
    "## üîó 4. Integrating NLP, Speech, and Vision\n",
    "\n",
    "Modern AI systems combine multiple modalities ‚Äî **language, audio, and vision** ‚Äî to create intelligent assistants and multimodal agents.\n",
    "\n",
    "Example pipeline:\n",
    "1. **User speaks a query ‚Üí STT ‚Üí Text**\n",
    "2. **NLP ‚Üí Intent Detection ‚Üí Context Understanding**\n",
    "3. **CV ‚Üí Visual Confirmation or Scene Understanding**\n",
    "4. **TTS ‚Üí Response Output (Human-like)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebabb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "g = Digraph('AIDomains', format='png')\n",
    "g.attr(rankdir='LR', size='9,4')\n",
    "g.attr('node', shape='box', style='filled', fillcolor='lightgreen')\n",
    "g.node('Speech', 'Speech Input (STT)')\n",
    "g.node('NLP', 'Text Understanding (NLP)')\n",
    "g.node('Vision', 'Scene Analysis (CV)')\n",
    "g.node('Response', 'Response Generation (TTS)')\n",
    "g.edges([('Speech','NLP'), ('NLP','Vision'), ('Vision','Response')])\n",
    "g.attr(label='Multimodal AI Pipeline: Voice ‚Üí Text ‚Üí Vision ‚Üí Voice')\n",
    "g.render('ai_domains_pipeline', view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad42d8",
   "metadata": {},
   "source": [
    "## üìò 5. Summary and Insights\n",
    "\n",
    "- **NLP** enables understanding and generation of human language.\n",
    "- **Speech AI** bridges human voice and digital understanding (STT ‚Üî TTS).\n",
    "- **Computer Vision** empowers perception and spatial awareness.\n",
    "- Combined, they form the foundation of **multimodal AI systems** ‚Äî powering assistants, self-driving cars, and smart robots.\n",
    "\n",
    "Next: In the following notebook, we'll explore how **AI integrates with Cloud, Edge, and IoT ecosystems** to deliver scalable real-world intelligence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
